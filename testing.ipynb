{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability based tree generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from checkpoint.\n",
      "Model loaded from model_output_10/model_size_small/sample_first_batch_True/sampling_mode_class_aware/checkpoint-1060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'figures/num_classes_10_sample_first_batch_True_sampling_mode_class_aware_checkpoint_number_1060_max_depth_7_top_k_2_min_norm_prob_0.5_branchpruned_False_globalpruned_False.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import json\n",
    "from graphviz import Digraph\n",
    "\n",
    "###############################################\n",
    "# Utility Functions for Model Inference\n",
    "###############################################\n",
    "\n",
    "\n",
    "def get_next_token_probs(sequence_ids):\n",
    "    \"\"\"\n",
    "    Given a tensor of input_ids (shape: [1, seq_len]),\n",
    "    returns the probability distribution for the next token.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence_ids, return_dict=True)\n",
    "    # Get logits for the last time step (shape: [1, vocab_size])\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Candidate Generation (Iterative Version)\n",
    "###############################################\n",
    "\n",
    "\n",
    "def generate_segment_candidates(initial_ids, top_k, min_norm_prob, max_segment_length):\n",
    "    \"\"\"\n",
    "    Generates segment candidates iteratively using a stack.\n",
    "    Each candidate is a sequence generated until a special token is reached.\n",
    "\n",
    "    Args:\n",
    "        initial_ids (torch.Tensor or list[int]): Starting sequence (tensor shape: [1, seq_len]).\n",
    "        top_k (int): Number of top tokens to consider at each extension.\n",
    "        min_norm_prob (float): Minimum per-token normalized probability threshold.\n",
    "        max_segment_length (int): Maximum number of tokens to add to the segment.\n",
    "\n",
    "    Returns:\n",
    "        candidates (list): List of tuples (candidate_ids, token_list, normalized_probability)\n",
    "                           where candidate_ids is a tensor, token_list is the list of token IDs\n",
    "                           (the full sequence from the start), and normalized_probability is the per-token normalized probability.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # Convert the initial_ids to a list of tokens.\n",
    "    if isinstance(initial_ids, torch.Tensor):\n",
    "        tokens = initial_ids.squeeze(0).tolist()\n",
    "    else:\n",
    "        tokens = initial_ids.copy()\n",
    "\n",
    "    # Each stack element is a tuple: (tokens_accum, current_log_prob, depth, visited)\n",
    "    # 'visited' is a frozenset of decoded sequences to avoid repetition.\n",
    "    stack = [(tokens, 0.0, 0, frozenset())]\n",
    "\n",
    "    while stack:\n",
    "        tokens_accum, current_log_prob, depth, visited = stack.pop()\n",
    "\n",
    "        # Terminate if we hit model's max positions or our max segment length.\n",
    "        if len(tokens_accum) >= model.config.n_positions or depth >= max_segment_length:\n",
    "            avg_log_prob = current_log_prob / (len(tokens_accum) if tokens_accum else 1)\n",
    "            norm_prob = math.exp(avg_log_prob)\n",
    "            current_ids = torch.tensor([tokens_accum], device=model.device)\n",
    "            candidates.append((current_ids, tokens_accum.copy(), norm_prob))\n",
    "            continue\n",
    "\n",
    "        # Build input tensor from the current token list.\n",
    "        current_ids = torch.tensor([tokens_accum], device=model.device)\n",
    "        probs = get_next_token_probs(current_ids)\n",
    "\n",
    "        # Get top_k tokens for extension.\n",
    "        topk_probs, topk_indices = torch.topk(probs.squeeze(0), top_k)\n",
    "        for prob, idx in zip(topk_probs, topk_indices):\n",
    "            token = idx.item()\n",
    "            new_log_prob = current_log_prob + math.log(prob.item())\n",
    "            new_tokens = tokens_accum + [token]\n",
    "            avg_log_prob = new_log_prob / len(new_tokens)\n",
    "            norm_prob = math.exp(avg_log_prob)\n",
    "\n",
    "            if norm_prob < min_norm_prob:\n",
    "                continue\n",
    "\n",
    "            # Decode the new sequence only for checking visited.\n",
    "            new_sequence = tokenizer.decode(new_tokens).strip()\n",
    "            if new_sequence in visited:\n",
    "                continue\n",
    "\n",
    "            new_visited = visited.union({new_sequence})\n",
    "            if token in SPECIAL_TOKEN_IDS:\n",
    "                new_ids = torch.tensor([new_tokens], device=model.device)\n",
    "                candidates.append((new_ids, new_tokens.copy(), norm_prob))\n",
    "            else:\n",
    "                stack.append((new_tokens, new_log_prob, depth + 1, new_visited))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Tree Building Functions (Segment-Level)\n",
    "###############################################\n",
    "\n",
    "\n",
    "def build_tree_segment(\n",
    "    sequence_ids,\n",
    "    max_depth,\n",
    "    current_depth=0,\n",
    "    top_k=5,\n",
    "    min_norm_prob=0.3,\n",
    "    max_segment_length=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively builds a tree where each branch is a segment generated until a special token is encountered.\n",
    "    A \"node\" represents the newly generated tokens between special tokens.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the decoded full sequence ('sequence'), the list of children nodes,\n",
    "        and (for non-root nodes) a 'segment_tokens' field that contains only the newly generated tokens.\n",
    "    \"\"\"\n",
    "    decoded = tokenizer.decode(sequence_ids[0])\n",
    "    # Get parent's tokens as a list.\n",
    "    parent_tokens = sequence_ids[0].tolist()\n",
    "\n",
    "    # Stop if max depth is reached or EOS is encountered.\n",
    "    if current_depth >= max_depth or tokenizer.eos_token_id in sequence_ids[0]:\n",
    "        return {\"sequence\": decoded, \"children\": []}\n",
    "\n",
    "    # Generate candidate segments.\n",
    "    segment_candidates = generate_segment_candidates(\n",
    "        sequence_ids,\n",
    "        top_k=top_k,\n",
    "        min_norm_prob=min_norm_prob,\n",
    "        max_segment_length=max_segment_length,\n",
    "    )\n",
    "\n",
    "    # Sort candidates by normalized probability (highest first) and select top_k.\n",
    "    segment_candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "    segment_candidates = segment_candidates[:top_k]\n",
    "\n",
    "    node = {\"sequence\": decoded, \"children\": []}\n",
    "    for new_ids, full_candidate_tokens, seg_norm_prob in segment_candidates:\n",
    "        # Extract only the newly generated tokens (i.e. the segment) by removing the parent's prefix.\n",
    "        segment = full_candidate_tokens[len(parent_tokens) :]\n",
    "        child = build_tree_segment(\n",
    "            new_ids,\n",
    "            max_depth,\n",
    "            current_depth + 1,\n",
    "            top_k,\n",
    "            min_norm_prob,\n",
    "            max_segment_length,\n",
    "        )\n",
    "        # Save only the segment tokens in the child.\n",
    "        child[\"segment_tokens\"] = segment\n",
    "        child[\"segment_norm_prob\"] = seg_norm_prob\n",
    "        node[\"children\"].append(child)\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def generate_hierarchical_tree_segment(\n",
    "    max_depth=5, top_k=5, min_norm_prob=0.3, max_segment_length=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Starts with the BOS token and builds a hierarchical tree using segment-level steps.\n",
    "    \"\"\"\n",
    "    initial_ids = tokenizer.encode(tokenizer.bos_token, return_tensors=\"pt\").to(device)\n",
    "    tree = build_tree_segment(\n",
    "        initial_ids,\n",
    "        max_depth=max_depth,\n",
    "        top_k=top_k,\n",
    "        min_norm_prob=min_norm_prob,\n",
    "        max_segment_length=max_segment_length,\n",
    "    )\n",
    "    return tree\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Visualization and Pruning Functions\n",
    "###############################################\n",
    "\n",
    "\n",
    "def visualize_tree_segment(\n",
    "    tree, graph=None, parent_id=None, counter=[0], include_special_tokens=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively adds nodes and edges from the tree to a Graphviz Digraph.\n",
    "    Each node displays only the segment tokens generated at that node (i.e. the tokens\n",
    "    added between special tokens), not the full sequence.\n",
    "\n",
    "    Args:\n",
    "      tree (dict): The tree dictionary.\n",
    "      graph (Digraph): The Graphviz graph instance.\n",
    "      parent_id (str): ID of the parent node.\n",
    "      counter (list): A mutable counter to generate unique node IDs.\n",
    "      include_special_tokens (bool): If True, special tokens (including <BOS>, <EOS>, <DOWNWARD>, etc.)\n",
    "                                     are shown; if False, they are filtered out.\n",
    "\n",
    "    Returns:\n",
    "      graph (Digraph): The updated Graphviz Digraph.\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Digraph()\n",
    "\n",
    "    node_id = f\"node_{counter[0]}\"\n",
    "    counter[0] += 1\n",
    "\n",
    "    # For non-root nodes, use the stored 'segment_tokens' (which is just the newly generated segment).\n",
    "    if \"segment_tokens\" in tree and tree[\"segment_tokens\"]:\n",
    "        tokens = tree[\"segment_tokens\"]\n",
    "        if not include_special_tokens:\n",
    "            tokens = [tok for tok in tokens if tok not in SPECIAL_TOKEN_IDS]\n",
    "        label = tokenizer.decode(tokens).strip()\n",
    "        if not label:\n",
    "            label = \"<empty>\"\n",
    "    else:\n",
    "        # For the root node, display the BOS token.\n",
    "        label = tokenizer.bos_token\n",
    "\n",
    "    # Optionally, append normalized probability.\n",
    "    if \"segment_norm_prob\" in tree:\n",
    "        label += f\"\\n(prob: {tree['segment_norm_prob']:.2f})\"\n",
    "\n",
    "    graph.node(node_id, label)\n",
    "\n",
    "    if parent_id is not None:\n",
    "        graph.edge(parent_id, node_id)\n",
    "\n",
    "    for child in tree.get(\"children\", []):\n",
    "        visualize_tree_segment(\n",
    "            child,\n",
    "            graph,\n",
    "            node_id,\n",
    "            counter,\n",
    "            include_special_tokens=include_special_tokens,\n",
    "        )\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def prune_tree_branch(node, seen_words=None):\n",
    "    \"\"\"\n",
    "    Recursively prunes the tree by removing branches where the \"word\"\n",
    "    (decoded from the segment tokens after filtering special tokens) is duplicated along the branch.\n",
    "\n",
    "    This enforces uniqueness within each branch only.\n",
    "    \"\"\"\n",
    "    print(\"Pruning branch-level duplicates...\")\n",
    "    if seen_words is None:\n",
    "        seen_words = set()\n",
    "\n",
    "    if \"segment_tokens\" in node and node[\"segment_tokens\"]:\n",
    "        word_tokens = [\n",
    "            tok for tok in node[\"segment_tokens\"] if tok not in SPECIAL_TOKEN_IDS\n",
    "        ]\n",
    "        word = tokenizer.decode(word_tokens).strip()\n",
    "    else:\n",
    "        word = tokenizer.bos_token\n",
    "\n",
    "    if word in seen_words:\n",
    "        return None\n",
    "\n",
    "    new_seen = seen_words.union({word})\n",
    "    pruned_children = []\n",
    "    for child in node.get(\"children\", []):\n",
    "        pruned_child = prune_tree_branch(child, new_seen)\n",
    "        if pruned_child is not None:\n",
    "            pruned_children.append(pruned_child)\n",
    "    node[\"children\"] = pruned_children\n",
    "    return node\n",
    "\n",
    "\n",
    "def prune_tree_global(node, global_seen=None):\n",
    "    \"\"\"\n",
    "    Recursively prunes the tree so that each unique \"word\" (decoded from the segment tokens)\n",
    "    appears only once in the entire tree.\n",
    "\n",
    "    This enforces global uniqueness.\n",
    "    \"\"\"\n",
    "    print(\"Pruning global duplicates...\")\n",
    "    if global_seen is None:\n",
    "        global_seen = set()\n",
    "\n",
    "    if \"segment_tokens\" in node and node[\"segment_tokens\"]:\n",
    "        word_tokens = [\n",
    "            tok for tok in node[\"segment_tokens\"] if tok not in SPECIAL_TOKEN_IDS\n",
    "        ]\n",
    "        word = tokenizer.decode(word_tokens).strip()\n",
    "    else:\n",
    "        word = tokenizer.bos_token\n",
    "\n",
    "    if word in global_seen:\n",
    "        return None\n",
    "\n",
    "    global_seen.add(word)\n",
    "\n",
    "    pruned_children = []\n",
    "    for child in node.get(\"children\", []):\n",
    "        pruned_child = prune_tree_global(child, global_seen)\n",
    "        if pruned_child is not None:\n",
    "            pruned_children.append(pruned_child)\n",
    "    node[\"children\"] = pruned_children\n",
    "    return node\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Configuration, Model Loading, and Run\n",
    "###############################################\n",
    "\n",
    "num_classes = 10\n",
    "checkpoint_number = 1060\n",
    "model_size = \"small\"\n",
    "sample_first_batch = True\n",
    "sampling_mode = \"class_aware\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(f\"./custom_tokenizer\")\n",
    "print(\"Tokenizer loaded from checkpoint.\")\n",
    "\n",
    "# --- Load Model ---\n",
    "checkpoint_dir = f\"model_output_{num_classes}/model_size_{model_size}/sample_first_batch_{sample_first_batch}/sampling_mode_{sampling_mode}/checkpoint-{checkpoint_number}\"\n",
    "config = GPT2Config.from_pretrained(checkpoint_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_dir, config=config)\n",
    "\n",
    "# Resize model embeddings to account for any added special tokens.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda\"  # Forcing CPU; change if you wish to use GPU.\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded from {checkpoint_dir}\")\n",
    "\n",
    "# --- Special Token IDs ---\n",
    "SPECIAL_TOKEN_IDS = {\n",
    "    tokenizer.convert_tokens_to_ids(\"<DOWNWARD>\"),\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.bos_token_id,\n",
    "}\n",
    "\n",
    "# --- Hierarchical Generation Parameters ---\n",
    "max_depth = 7\n",
    "top_k = 2\n",
    "min_norm_prob = 0.5\n",
    "\n",
    "# --- Run the Hierarchical Generation (Segment-Level) ---\n",
    "tree = generate_hierarchical_tree_segment(\n",
    "    max_depth=max_depth,\n",
    "    top_k=top_k,\n",
    "    min_norm_prob=min_norm_prob,\n",
    "    max_segment_length=20,\n",
    ")\n",
    "\n",
    "# --- Optional Pruning ---\n",
    "# Toggle these flags to apply branch-level and/or global pruning.\n",
    "apply_branch_pruning = False  # Set to True to apply branch-level pruning.\n",
    "apply_global_pruning = False  # Set to True to apply global pruning.\n",
    "\n",
    "if apply_branch_pruning:\n",
    "    tree = prune_tree_branch(tree)\n",
    "if apply_global_pruning:\n",
    "    tree = prune_tree_global(tree)\n",
    "\n",
    "# Create a tag to include in the filename indicating which pruning was applied.\n",
    "pruning_tag = f\"branchpruned_{apply_branch_pruning}_globalpruned_{apply_global_pruning}\"\n",
    "\n",
    "# --- Visualize and Render the Tree ---\n",
    "# Set include_special_tokens to True to show special tokens (like <EOS>) in node labels.\n",
    "graph = visualize_tree_segment(tree, include_special_tokens=False)\n",
    "output_filename = (\n",
    "    f\"./figures/num_classes_{num_classes}_sample_first_batch_{sample_first_batch}_\"\n",
    "    f\"sampling_mode_{sampling_mode}_checkpoint_number_{checkpoint_number}_max_depth_{max_depth}_\"\n",
    "    f\"top_k_{top_k}_min_norm_prob_{min_norm_prob}_{pruning_tag}\"\n",
    ")\n",
    "graph.render(output_filename, view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
