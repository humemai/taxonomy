{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 919 TSV files.\n",
      "Loaded 507745 total paths.\n",
      "\n",
      "Graph Analysis Results:\n",
      "Total graph has 2757 nodes and 4964 edges\n",
      "Found 11 separate connected components\n",
      "Component sizes: [2718, 7, 7, 6, 6, 3, 2, 2, 2, 2] (showing top 10)\n",
      "Component 1: 2718 nodes, 4935 edges\n",
      "Component 2: 7 nodes, 6 edges\n",
      "Component 3: 7 nodes, 6 edges\n",
      "Component 4: 6 nodes, 5 edges\n",
      "Component 5: 6 nodes, 5 edges\n",
      "\n",
      "Analyzing the largest component:\n",
      "Largest component has 2718 nodes and 4935 edges\n",
      "Graph saved to largest_component.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'largest_component.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "with open(\"./entityid2label.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entityid2label = json.load(f)\n",
    "\n",
    "\n",
    "def get_tsv_paths(num_classes, sample_first_batch=False):\n",
    "    \"\"\"Load TSV file paths based on class counts.\"\"\"\n",
    "    with open(\"process_p31_p279/class_counts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        class_counts = json.load(f)\n",
    "    starting_entities = set(list(class_counts.keys())[:num_classes])\n",
    "    tsv_paths_by_class = {}\n",
    "\n",
    "    for path in glob(f\"./extracted_paths/*/*.tsv\"):\n",
    "        class_dir = os.path.basename(os.path.dirname(path))\n",
    "        if class_dir in starting_entities:\n",
    "            tsv_paths_by_class.setdefault(class_dir, []).append(path)\n",
    "\n",
    "    tsv_paths = []\n",
    "    if sample_first_batch:\n",
    "        for class_dir, paths in tsv_paths_by_class.items():\n",
    "            batch1_files = [p for p in paths if \"batch_1\" in os.path.basename(p)]\n",
    "            if batch1_files:\n",
    "                tsv_paths.append(batch1_files[0])\n",
    "            else:\n",
    "                tsv_paths.append(paths[0])\n",
    "    else:\n",
    "        for paths in tsv_paths_by_class.values():\n",
    "            tsv_paths.extend(paths)\n",
    "\n",
    "    print(f\"Found {len(tsv_paths)} TSV files.\")\n",
    "    return tsv_paths\n",
    "\n",
    "\n",
    "def read_tsv(path):\n",
    "    \"\"\"Read paths from a TSV file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as tf:\n",
    "        for line in tf:\n",
    "            path_entities = line.strip().split(\"\\t\")\n",
    "            yield path_entities\n",
    "\n",
    "\n",
    "def load_paths(num_classes, sample_first_batch):\n",
    "    \"\"\"Load all paths from TSV files.\"\"\"\n",
    "    tsv_paths = get_tsv_paths(num_classes, sample_first_batch)\n",
    "    paths = []\n",
    "    for tsv_path in tsv_paths:\n",
    "        for path in read_tsv(tsv_path):\n",
    "            paths.append(path)\n",
    "    print(f\"Loaded {len(paths)} total paths.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def enrich_paths(paths, entityid2label):\n",
    "    \"\"\"Enrich paths with labels.\"\"\"\n",
    "    enriched_paths = []\n",
    "    for path in paths:\n",
    "        enriched_path = []\n",
    "        for node in path:\n",
    "            enriched_path.append(f\"{entityid2label[node]}\\n({node})\")\n",
    "        enriched_paths.append(enriched_path)\n",
    "    return enriched_paths\n",
    "\n",
    "\n",
    "def create_graph_from_paths(paths):\n",
    "    \"\"\"Create a directed graph from paths.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges from all paths\n",
    "    for path in paths:\n",
    "        for i in range(len(path) - 1):\n",
    "            G.add_edge(path[i], path[i + 1])\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def analyze_graphs(G):\n",
    "    \"\"\"Find all connected components and analyze them.\"\"\"\n",
    "    # Find weakly connected components\n",
    "    components = list(nx.weakly_connected_components(G))\n",
    "    component_sizes = [len(comp) for comp in components]\n",
    "\n",
    "    print(f\"\\nGraph Analysis Results:\")\n",
    "    print(\n",
    "        f\"Total graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\"\n",
    "    )\n",
    "    print(f\"Found {len(components)} separate connected components\")\n",
    "    print(\n",
    "        f\"Component sizes: {sorted(component_sizes, reverse=True)[:10]} (showing top 10)\"\n",
    "    )\n",
    "\n",
    "    # Extract component subgraphs for further analysis\n",
    "    subgraphs = []\n",
    "    for i, component in enumerate(sorted(components, key=len, reverse=True)):\n",
    "        subgraph = G.subgraph(component).copy()\n",
    "        subgraphs.append(subgraph)\n",
    "        if i < 5:  # Print stats for the 5 largest components\n",
    "            print(\n",
    "                f\"Component {i+1}: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\"\n",
    "            )\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "# 1a. Read paths\n",
    "paths = load_paths(num_classes=100, sample_first_batch=False)\n",
    "\n",
    "# 1b. Enrich paths with labels\n",
    "paths = enrich_paths(paths, entityid2label)\n",
    "\n",
    "# 2. Create graph from paths\n",
    "G = create_graph_from_paths(paths)\n",
    "\n",
    "# 3. Find N graphs and analyze them\n",
    "subgraphs = analyze_graphs(G)\n",
    "\n",
    "# 4. Choose the biggest component\n",
    "largest_graph = subgraphs[0]\n",
    "print(f\"\\nAnalyzing the largest component:\")\n",
    "print(\n",
    "    f\"Largest component has {largest_graph.number_of_nodes()} nodes and {largest_graph.number_of_edges()} edges\"\n",
    ")\n",
    "\n",
    "\n",
    "def save_graph_to_json(G, filename):\n",
    "    \"\"\"Save a NetworkX graph as a JSON file in node-link format.\"\"\"\n",
    "    # Add labels to nodes if they don't have them\n",
    "    for node in G.nodes():\n",
    "        if \"label\" not in G.nodes[node]:\n",
    "            G.nodes[node][\"label\"] = str(node)\n",
    "\n",
    "    # Convert the graph to node-link format\n",
    "    data = nx.node_link_data(G)\n",
    "\n",
    "    # Save to file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Graph saved to {filename}\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "save_graph_to_json(largest_graph, \"largest_component.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
