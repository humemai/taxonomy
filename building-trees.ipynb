{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability based tree generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from checkpoint.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'model_output_10/model_size_small/sample_first_batch_True/sampling_mode_class_aware/checkpoint-1060'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'model_output_10/model_size_small/sample_first_batch_True/sampling_mode_class_aware/checkpoint-1060'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 324\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# --- Load Model ---\u001b[39;00m\n\u001b[1;32m    323\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_output_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sample_first_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_first_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sampling_mode_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 324\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint_dir, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Resize model embeddings to account for any added special tokens.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:551\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 551\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    553\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key]\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:591\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:650\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/.virtualenvs/dev-python3.11/lib/python3.11/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'model_output_10/model_size_small/sample_first_batch_True/sampling_mode_class_aware/checkpoint-1060'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import json\n",
    "from graphviz import Digraph\n",
    "\n",
    "###############################################\n",
    "# Utility Functions for Model Inference\n",
    "###############################################\n",
    "\n",
    "\n",
    "def get_next_token_probs(sequence_ids):\n",
    "    \"\"\"\n",
    "    Given a tensor of input_ids (shape: [1, seq_len]),\n",
    "    returns the probability distribution for the next token.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence_ids, return_dict=True)\n",
    "    # Get logits for the last time step (shape: [1, vocab_size])\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Candidate Generation (Iterative Version)\n",
    "###############################################\n",
    "\n",
    "\n",
    "def generate_segment_candidates(initial_ids, top_k, min_norm_prob, max_segment_length):\n",
    "    \"\"\"\n",
    "    Generates segment candidates iteratively using a stack.\n",
    "    Each candidate is a sequence generated until a special token is reached.\n",
    "\n",
    "    Args:\n",
    "        initial_ids (torch.Tensor or list[int]): Starting sequence (tensor shape: [1, seq_len]).\n",
    "        top_k (int): Number of top tokens to consider at each extension.\n",
    "        min_norm_prob (float): Minimum per-token normalized probability threshold.\n",
    "        max_segment_length (int): Maximum number of tokens to add to the segment.\n",
    "\n",
    "    Returns:\n",
    "        candidates (list): List of tuples (candidate_ids, token_list, normalized_probability)\n",
    "                           where candidate_ids is a tensor, token_list is the list of token IDs\n",
    "                           (the full sequence from the start), and normalized_probability is the per-token normalized probability.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # Convert the initial_ids to a list of tokens.\n",
    "    if isinstance(initial_ids, torch.Tensor):\n",
    "        tokens = initial_ids.squeeze(0).tolist()\n",
    "    else:\n",
    "        tokens = initial_ids.copy()\n",
    "\n",
    "    # Each stack element is a tuple: (tokens_accum, current_log_prob, depth, visited)\n",
    "    # 'visited' is a frozenset of decoded sequences to avoid repetition.\n",
    "    stack = [(tokens, 0.0, 0, frozenset())]\n",
    "\n",
    "    while stack:\n",
    "        tokens_accum, current_log_prob, depth, visited = stack.pop()\n",
    "\n",
    "        # Terminate if we hit model's max positions or our max segment length.\n",
    "        if len(tokens_accum) >= model.config.n_positions or depth >= max_segment_length:\n",
    "            avg_log_prob = current_log_prob / (len(tokens_accum) if tokens_accum else 1)\n",
    "            norm_prob = math.exp(avg_log_prob)\n",
    "            current_ids = torch.tensor([tokens_accum], device=model.device)\n",
    "            candidates.append((current_ids, tokens_accum.copy(), norm_prob))\n",
    "            continue\n",
    "\n",
    "        # Build input tensor from the current token list.\n",
    "        current_ids = torch.tensor([tokens_accum], device=model.device)\n",
    "        probs = get_next_token_probs(current_ids)\n",
    "\n",
    "        # Get top_k tokens for extension.\n",
    "        topk_probs, topk_indices = torch.topk(probs.squeeze(0), top_k)\n",
    "        for prob, idx in zip(topk_probs, topk_indices):\n",
    "            token = idx.item()\n",
    "            new_log_prob = current_log_prob + math.log(prob.item())\n",
    "            new_tokens = tokens_accum + [token]\n",
    "            avg_log_prob = new_log_prob / len(new_tokens)\n",
    "            norm_prob = math.exp(avg_log_prob)\n",
    "\n",
    "            if norm_prob < min_norm_prob:\n",
    "                continue\n",
    "\n",
    "            # Decode the new sequence only for checking visited.\n",
    "            new_sequence = tokenizer.decode(new_tokens).strip()\n",
    "            if new_sequence in visited:\n",
    "                continue\n",
    "\n",
    "            new_visited = visited.union({new_sequence})\n",
    "            if token in SPECIAL_TOKEN_IDS:\n",
    "                new_ids = torch.tensor([new_tokens], device=model.device)\n",
    "                candidates.append((new_ids, new_tokens.copy(), norm_prob))\n",
    "            else:\n",
    "                stack.append((new_tokens, new_log_prob, depth + 1, new_visited))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Tree Building Functions (Segment-Level)\n",
    "###############################################\n",
    "\n",
    "\n",
    "def build_tree_segment(\n",
    "    sequence_ids,\n",
    "    max_depth,\n",
    "    current_depth=0,\n",
    "    top_k=5,\n",
    "    min_norm_prob=0.3,\n",
    "    max_segment_length=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively builds a tree where each branch is a segment generated until a special token is encountered.\n",
    "    A \"node\" represents the newly generated tokens between special tokens.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the decoded full sequence ('sequence'), the list of children nodes,\n",
    "        and (for non-root nodes) a 'segment_tokens' field that contains only the newly generated tokens.\n",
    "    \"\"\"\n",
    "    decoded = tokenizer.decode(sequence_ids[0])\n",
    "    # Get parent's tokens as a list.\n",
    "    parent_tokens = sequence_ids[0].tolist()\n",
    "\n",
    "    # Stop if max depth is reached or EOS is encountered.\n",
    "    if current_depth >= max_depth or tokenizer.eos_token_id in sequence_ids[0]:\n",
    "        return {\"sequence\": decoded, \"children\": []}\n",
    "\n",
    "    # Generate candidate segments.\n",
    "    segment_candidates = generate_segment_candidates(\n",
    "        sequence_ids,\n",
    "        top_k=top_k,\n",
    "        min_norm_prob=min_norm_prob,\n",
    "        max_segment_length=max_segment_length,\n",
    "    )\n",
    "\n",
    "    # Sort candidates by normalized probability (highest first) and select top_k.\n",
    "    segment_candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "    segment_candidates = segment_candidates[:top_k]\n",
    "\n",
    "    node = {\"sequence\": decoded, \"children\": []}\n",
    "    for new_ids, full_candidate_tokens, seg_norm_prob in segment_candidates:\n",
    "        # Extract only the newly generated tokens (i.e. the segment) by removing the parent's prefix.\n",
    "        segment = full_candidate_tokens[len(parent_tokens) :]\n",
    "        child = build_tree_segment(\n",
    "            new_ids,\n",
    "            max_depth,\n",
    "            current_depth + 1,\n",
    "            top_k,\n",
    "            min_norm_prob,\n",
    "            max_segment_length,\n",
    "        )\n",
    "        # Save only the segment tokens in the child.\n",
    "        child[\"segment_tokens\"] = segment\n",
    "        child[\"segment_norm_prob\"] = seg_norm_prob\n",
    "        node[\"children\"].append(child)\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def generate_hierarchical_tree_segment(\n",
    "    max_depth=5, top_k=5, min_norm_prob=0.3, max_segment_length=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Starts with the BOS token and builds a hierarchical tree using segment-level steps.\n",
    "    \"\"\"\n",
    "    initial_ids = tokenizer.encode(tokenizer.bos_token, return_tensors=\"pt\").to(device)\n",
    "    tree = build_tree_segment(\n",
    "        initial_ids,\n",
    "        max_depth=max_depth,\n",
    "        top_k=top_k,\n",
    "        min_norm_prob=min_norm_prob,\n",
    "        max_segment_length=max_segment_length,\n",
    "    )\n",
    "    return tree\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Visualization and Pruning Functions\n",
    "###############################################\n",
    "\n",
    "\n",
    "def visualize_tree_segment(\n",
    "    tree, graph=None, parent_id=None, counter=[0], include_special_tokens=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively adds nodes and edges from the tree to a Graphviz Digraph.\n",
    "    Each node displays only the segment tokens generated at that node (i.e. the tokens\n",
    "    added between special tokens), not the full sequence.\n",
    "\n",
    "    Args:\n",
    "      tree (dict): The tree dictionary.\n",
    "      graph (Digraph): The Graphviz graph instance.\n",
    "      parent_id (str): ID of the parent node.\n",
    "      counter (list): A mutable counter to generate unique node IDs.\n",
    "      include_special_tokens (bool): If True, special tokens (including <BOS>, <EOS>, <DOWNWARD>, etc.)\n",
    "                                     are shown; if False, they are filtered out.\n",
    "\n",
    "    Returns:\n",
    "      graph (Digraph): The updated Graphviz Digraph.\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Digraph()\n",
    "\n",
    "    node_id = f\"node_{counter[0]}\"\n",
    "    counter[0] += 1\n",
    "\n",
    "    # For non-root nodes, use the stored 'segment_tokens' (which is just the newly generated segment).\n",
    "    if \"segment_tokens\" in tree and tree[\"segment_tokens\"]:\n",
    "        tokens = tree[\"segment_tokens\"]\n",
    "        if not include_special_tokens:\n",
    "            tokens = [tok for tok in tokens if tok not in SPECIAL_TOKEN_IDS]\n",
    "        label = tokenizer.decode(tokens).strip()\n",
    "        if not label:\n",
    "            label = \"<empty>\"\n",
    "    else:\n",
    "        # For the root node, display the BOS token.\n",
    "        label = tokenizer.bos_token\n",
    "\n",
    "    # Optionally, append normalized probability.\n",
    "    if \"segment_norm_prob\" in tree:\n",
    "        label += f\"\\n(prob: {tree['segment_norm_prob']:.2f})\"\n",
    "\n",
    "    graph.node(node_id, label)\n",
    "\n",
    "    if parent_id is not None:\n",
    "        graph.edge(parent_id, node_id)\n",
    "\n",
    "    for child in tree.get(\"children\", []):\n",
    "        visualize_tree_segment(\n",
    "            child,\n",
    "            graph,\n",
    "            node_id,\n",
    "            counter,\n",
    "            include_special_tokens=include_special_tokens,\n",
    "        )\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def prune_tree_branch(node, seen_words=None):\n",
    "    \"\"\"\n",
    "    Recursively prunes the tree by removing branches where the \"word\"\n",
    "    (decoded from the segment tokens after filtering special tokens) is duplicated along the branch.\n",
    "\n",
    "    This enforces uniqueness within each branch only.\n",
    "    \"\"\"\n",
    "    print(\"Pruning branch-level duplicates...\")\n",
    "    if seen_words is None:\n",
    "        seen_words = set()\n",
    "\n",
    "    if \"segment_tokens\" in node and node[\"segment_tokens\"]:\n",
    "        word_tokens = [\n",
    "            tok for tok in node[\"segment_tokens\"] if tok not in SPECIAL_TOKEN_IDS\n",
    "        ]\n",
    "        word = tokenizer.decode(word_tokens).strip()\n",
    "    else:\n",
    "        word = tokenizer.bos_token\n",
    "\n",
    "    if word in seen_words:\n",
    "        return None\n",
    "\n",
    "    new_seen = seen_words.union({word})\n",
    "    pruned_children = []\n",
    "    for child in node.get(\"children\", []):\n",
    "        pruned_child = prune_tree_branch(child, new_seen)\n",
    "        if pruned_child is not None:\n",
    "            pruned_children.append(pruned_child)\n",
    "    node[\"children\"] = pruned_children\n",
    "    return node\n",
    "\n",
    "\n",
    "def prune_tree_global(node, global_seen=None):\n",
    "    \"\"\"\n",
    "    Recursively prunes the tree so that each unique \"word\" (decoded from the segment tokens)\n",
    "    appears only once in the entire tree.\n",
    "\n",
    "    This enforces global uniqueness.\n",
    "    \"\"\"\n",
    "    print(\"Pruning global duplicates...\")\n",
    "    if global_seen is None:\n",
    "        global_seen = set()\n",
    "\n",
    "    if \"segment_tokens\" in node and node[\"segment_tokens\"]:\n",
    "        word_tokens = [\n",
    "            tok for tok in node[\"segment_tokens\"] if tok not in SPECIAL_TOKEN_IDS\n",
    "        ]\n",
    "        word = tokenizer.decode(word_tokens).strip()\n",
    "    else:\n",
    "        word = tokenizer.bos_token\n",
    "\n",
    "    if word in global_seen:\n",
    "        return None\n",
    "\n",
    "    global_seen.add(word)\n",
    "\n",
    "    pruned_children = []\n",
    "    for child in node.get(\"children\", []):\n",
    "        pruned_child = prune_tree_global(child, global_seen)\n",
    "        if pruned_child is not None:\n",
    "            pruned_children.append(pruned_child)\n",
    "    node[\"children\"] = pruned_children\n",
    "    return node\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Configuration, Model Loading, and Run\n",
    "###############################################\n",
    "\n",
    "num_classes = 10\n",
    "checkpoint_number = 1060\n",
    "model_size = \"small\"\n",
    "sample_first_batch = True\n",
    "sampling_mode = \"class_aware\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(f\"./custom_tokenizer\")\n",
    "print(\"Tokenizer loaded from checkpoint.\")\n",
    "\n",
    "# --- Load Model ---\n",
    "checkpoint_dir = f\"model_output_{num_classes}/model_size_{model_size}/sample_first_batch_{sample_first_batch}/sampling_mode_{sampling_mode}/checkpoint-{checkpoint_number}\"\n",
    "config = GPT2Config.from_pretrained(checkpoint_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_dir, config=config)\n",
    "\n",
    "# Resize model embeddings to account for any added special tokens.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda\"  # Forcing CPU; change if you wish to use GPU.\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded from {checkpoint_dir}\")\n",
    "\n",
    "# --- Special Token IDs ---\n",
    "SPECIAL_TOKEN_IDS = {\n",
    "    tokenizer.convert_tokens_to_ids(\"<DOWNWARD>\"),\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.bos_token_id,\n",
    "}\n",
    "\n",
    "# --- Hierarchical Generation Parameters ---\n",
    "max_depth = 7\n",
    "top_k = 2\n",
    "min_norm_prob = 0.5\n",
    "\n",
    "# --- Run the Hierarchical Generation (Segment-Level) ---\n",
    "tree = generate_hierarchical_tree_segment(\n",
    "    max_depth=max_depth,\n",
    "    top_k=top_k,\n",
    "    min_norm_prob=min_norm_prob,\n",
    "    max_segment_length=20,\n",
    ")\n",
    "\n",
    "# --- Optional Pruning ---\n",
    "# Toggle these flags to apply branch-level and/or global pruning.\n",
    "apply_branch_pruning = False  # Set to True to apply branch-level pruning.\n",
    "apply_global_pruning = False  # Set to True to apply global pruning.\n",
    "\n",
    "if apply_branch_pruning:\n",
    "    tree = prune_tree_branch(tree)\n",
    "if apply_global_pruning:\n",
    "    tree = prune_tree_global(tree)\n",
    "\n",
    "# Create a tag to include in the filename indicating which pruning was applied.\n",
    "pruning_tag = f\"branchpruned_{apply_branch_pruning}_globalpruned_{apply_global_pruning}\"\n",
    "\n",
    "# --- Visualize and Render the Tree ---\n",
    "# Set include_special_tokens to True to show special tokens (like <EOS>) in node labels.\n",
    "graph = visualize_tree_segment(tree, include_special_tokens=False)\n",
    "output_filename = (\n",
    "    f\"./tree_figures/num_classes_{num_classes}_sample_first_batch_{sample_first_batch}_\"\n",
    "    f\"sampling_mode_{sampling_mode}_checkpoint_number_{checkpoint_number}_max_depth_{max_depth}_\"\n",
    "    f\"top_k_{top_k}_min_norm_prob_{min_norm_prob}_{pruning_tag}\"\n",
    ")\n",
    "graph.render(output_filename, view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
